## 算法评价指标

#### 名词解释

|TP|FP|FN|TN|
|:---:|:---:|:---:|:---:|
|真正例|假正例|假负例|真负例|

TP —— True Positive（真正）被模型预测为正的正样本

FP —— False Positive （假正）被模型预测为正的负样本

TN —— True Negative（真负）被模型预测为负的负样本

FN —— False Negative（假负）被模型预测为负的正样本

#### 1. 召回率

$$Recall=\frac{TP}{TP+FN} $$

#### 2. 准确率
$$ Precision=\frac{TP}{TP+FP} $$

#### 3. F-Measure（精确率和召回率的调和均值）

$$ F=\frac{(1+α^2)P*R}{α^2(P+R)}$$

当α=1时，即为最常见的F1指标，F1越大表明算法性能越好。

$$ F1=\frac{2*P*R}{P+R} $$

#### 4. ROC曲线

ROC曲线描述召回率TPR随误检率FPR变化的曲线。

TPR 代表能将正例分对的概率，即召回率，聚焦正样本。

FPR 代表将负例错分为正例的概率，聚焦负样本。
$$ FPR=\frac{FP}{TN+FP} $$

特点：ROC曲线不会随着类别分布的改变而改变，但这在某种程度上也是其缺点。因为当负例N增加了很多时，而曲线却没变，这等于产生了大量FP，检测准确率下降。对于像信息检索中这种主要关心正例的预测准确性的场景，ROC就无法很好描述其性能了。

![](../materials/ROC.png)

ROC曲线下方积分面积（AUC）越大，表明分类器性能越好；

虽然ROC曲线相比较于Precision和Recall等衡量指标更加合理，但是其在高不平衡数据条件下的的表现仍然过于理想，不能够很好的展示实际情况。在类别不平衡的背景下，负例的数目众多致使FPR的增长不明显，导致ROC曲线呈现一个过分乐观的效果估计。

##### 如何绘制ROC曲线

假设已经得出一系列样本被划分为正类的概率，将Scores从高到低排序，并依次将Score值作为阈值threshold，当测试样本属于正样本的概率大于或等于这个threshold时，为正样本，否则为负样本。可以得到一组FPR和TPR，即ROC曲线上的一点。将它们画在ROC曲线的结果如下图：

![](../materials/roc_draw.jpg)

#### 5. PR曲线

描述准确率Precision随召回率Recall变化的曲线，绘制方法同ROC曲线。

和ROC曲线的对比：

正负样本的分布失衡的时候，ROC曲线保持不变，而PR曲线会产生很大的变化。(a)（b）分别是正反例相等的时候的ROC曲线和PR曲线,（c）（d）分别是负例：正例=10:1的ROC曲线和PR曲线

![](../materials/roc_pr.png)

在正负失衡的情况下（负样本大于正样本），从ROC曲线看分类器的表现仍然较好（图c），然而从PR曲线来看，分类器就表现的很差。

而PR曲线的两个指标都聚焦于正例。类别不平衡问题中由于主要关心正例，所以在此情况下PR曲线被广泛认为优于ROC曲线。
